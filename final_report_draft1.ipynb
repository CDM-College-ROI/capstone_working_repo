{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3403e29",
   "metadata": {},
   "source": [
    "# Return on Investment of US College Majors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2356e2",
   "metadata": {},
   "source": [
    "By Mijail Mariano, Chenchen Feng, & David Schneemann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2dee1",
   "metadata": {},
   "source": [
    "## Project Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b905ed",
   "metadata": {},
   "source": [
    "Our goal with this project is to predict return on investment for US college undergraduate majors. We use US Department of Education data to identify statistically significant metrics in order to predict this \"cost-to-earnings\" ratio of return on investment. We explore how potential features within the data produce certain return on investment ratios aand provide insight into why and how these factors contribute to this target variable. With this information and the following recommendations, we intend to provide current and prospective students a greater understanding of the value and trade-offs tied to aa given college major and its potential career paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7600c4a",
   "metadata": {},
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38607fc",
   "metadata": {},
   "source": [
    "The cost of attending college and its associated benefits and drawbacks are primary topics of conversation for young people today. The actual return on investment of such an endeavor is difficult to quantify and leaves prospective students guessing when choosing a college and degree to pursue and whether college is even the right choice. This analysis hopes to provide more clarity for students in making these key decisions about their future.\n",
    "\n",
    "In order to more accurately predict return on investment, we will analyze the attributes (features) of college majors. We gathered data for all US colleges from the US Department of Education College Scorecard and US Census earnings data provided by University of Minnesota's IPUMS database. \n",
    "Our chosen dataset includes all gathered data for the academic years of 2017-2018 and 2018-2019 and earnings data from 2019 spanning 1% of the US population. \n",
    "In total, our initial dataset contains 225,000 records and 3000+ features. \n",
    "Our final cleaned and prepared dataset contains roughly the same number of records but narrows to roughly 100 key features. \n",
    "\n",
    "Our approach to this project involves clustering by groups of these features in order to best predict return on investment while simultaneously providing depth and insight into what features most affect return on investment and overall college outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959daad",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08ef34",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "## Clustering Questions\n",
    "\n",
    "#### 1. Does bedroom, bathroom, and garage space count affect log error?\n",
    "\n",
    "#### 2.  Does location, latitude, and longitude affect log error?\n",
    "    \n",
    "#### 3. Do sqft, lot_sq_ft, and bath_bed_ratio affect log error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a38b1",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------\n",
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c28e7c",
   "metadata": {},
   "source": [
    "In order to effectively meet our goals, the following module imports are required. \\\n",
    "Below is an extensive list of all modules I imported and used to create and complete the desired analysis for Zillow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9de8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "# default pandas decimal number display format\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_regression, SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kruskal\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import csv\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8b492",
   "metadata": {},
   "source": [
    "| Variable      | Meaning |\n",
    "| ----------- | ----------- |\n",
    "| logerror      | The measured log error of a home       |\n",
    "| home_value      | The total tax assessed value of the parcel       |\n",
    "| bedrooms   | The total number of bedrooms in a home        |\n",
    "| bathrooms      | The total number of bathrooms in a home       |\n",
    "| garage_spaces      | The total number of car slots in a garage       |\n",
    "| year_built      | The year the home was built       |\n",
    "| age      | The age of the home       |\n",
    "| location      | Location of a home by county      |\n",
    "| sq_ft      | The total square feet of a home       |\n",
    "| lot_sq_ft      | The total square feet of a property lot       |\n",
    "| latitude   | Location using the latitudenal metric        |\n",
    "| longitude      | Location using the longitudenal metric       |\n",
    "| bath_bed_ratio      | Ratio of bathrooms to bedrooms of a home       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e9ef6",
   "metadata": {},
   "source": [
    "## Acquire Zillow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9ebc5",
   "metadata": {},
   "source": [
    "##### We acquire our data by utilizing the acquire.py file.\n",
    "This file pulls selected features from FieldOfStudyData1718_1819_PP and joins them with MERGED2018_19_PP. \\\n",
    "We then join this merged dataframe with select earnings data from IPUMS dataset; namely years 2017, 2018, & 2019. \\\n",
    "Our resulting table returns 56,079 entries of data with the following attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2ecc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape: (0, 139)\n",
      "Shape of resulting df:(0, 139)\n"
     ]
    }
   ],
   "source": [
    "# Calling my acquire.py file and utilizing its function,assigning the output to df\n",
    "df = acquire.get_bach_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4654321",
   "metadata": {},
   "source": [
    "## Prepare Zillow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4d424",
   "metadata": {},
   "source": [
    "##### We prepare our data by utilizing the prepare.py file.\n",
    "This file:\n",
    "- Handles null values\n",
    "- Converts some variables to integers for optimization\n",
    "- Cleans variables, including dropping numerous extraneous features along with renaming columns \n",
    "- Includes robust feature engineering including \n",
    "    - Condensing majors into predominant major_categories and \n",
    "    - Iterating Family Income brackets\n",
    "    - Engineering our target variable ROI (5, 10, & 20 years) by earnings data and net price vars\n",
    "- Splits prepared df into train, validate, test \n",
    "- Handles outliers through a process called \"capping\" via the \"winsorize\" method\n",
    "- Manual imputation of select features\n",
    "- Utilizes an iterative imputer to programmatically handle remaining missing values\n",
    "\n",
    "Our resulting dataframes are ready for exploration and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape: (0, 117)\n",
      "dataframe shape: (0, 127)\n",
      "Shape of resulting df:(0, 127)\n"
     ]
    }
   ],
   "source": [
    "df = prepare.clean_college_df(df)\n",
    "df = prepare.clean_high_percentage_nulls(df)\n",
    "df = prepare.obtain_target_variables(df)\n",
    "print(f'Shape of resulting df:{df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating income brackets\n",
    "\n",
    "income_0_30000 = [\n",
    "'other_fam_income_0_30000',\n",
    " 'private_fam_income_0_30000',\n",
    " 'program_fam_income_0_30000',\n",
    " 'pub_fam_income_0_30000']\n",
    "\n",
    "income_30001_48000 = [\n",
    " 'other_fam_income_30001_48000',\n",
    " 'private_fam_income_30001_48000',\n",
    " 'program_fam_income_30001_48000',\n",
    " 'pub_fam_income_30001_48000']\n",
    "\n",
    "income_48001_75000 = [\n",
    "'other_fam_income_48001_75000',\n",
    "'private_fam_income_48001_75000',\n",
    "'program_fam_income_48001_75000',\n",
    "'pub_fam_income_48001_75000']\n",
    "\n",
    "income_75001_110000 = [\n",
    "'other_fam_income_75001_110000',\n",
    "'private_fam_income_75001_110000',\n",
    "'program_fam_income_75001_110000',\n",
    "'pub_fam_income_75001_110000']\n",
    "\n",
    "income_over_110000 = [\n",
    "'other_fam_income_over_110000',\n",
    "'private_fam_income_over_110000',\n",
    "'program_fam_income_over_110000',\n",
    "'pub_fam_income_over_110000']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare.get_fam_income_col(df, income_0_30000, \"fam_income_0_30000\")\n",
    "df = prepare.get_fam_income_col(df, income_30001_48000, \"fam_income_30001_48000\")\n",
    "df = prepare.get_fam_income_col(df, income_48001_75000, \"fam_income_48001_75000\")\n",
    "df = prepare.get_fam_income_col(df, income_75001_110000, \"fam_income_75001_110000\")\n",
    "df = prepare.get_fam_income_col(df, income_over_110000, \"fam_income_over_110000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating region category feature from `state_post_code`\n",
    "df['us_region'] = df.apply(lambda row: prepare.label_states(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/davidschneemann/codeup/capstone_working_repo/final_report_draft1.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/davidschneemann/codeup/capstone_working_repo/final_report_draft1.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train, validate, test \u001b[39m=\u001b[39m prepare\u001b[39m.\u001b[39;49msplit_data(df)\n",
      "File \u001b[0;32m~/codeup/capstone_working_repo/prepare.py:605\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_data\u001b[39m(df):\n\u001b[0;32m--> 605\u001b[0m     train_and_validate, test \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m    606\u001b[0m         df, \n\u001b[1;32m    607\u001b[0m         test_size \u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m, \n\u001b[1;32m    608\u001b[0m         random_state \u001b[39m=\u001b[39;49m \u001b[39m123\u001b[39;49m,\n\u001b[1;32m    609\u001b[0m         stratify \u001b[39m=\u001b[39;49m df[\u001b[39m\"\u001b[39;49m\u001b[39mmajor_category\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    611\u001b[0m     train, validate \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m    612\u001b[0m         train_and_validate,\n\u001b[1;32m    613\u001b[0m         test_size \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m,\n\u001b[1;32m    614\u001b[0m         random_state \u001b[39m=\u001b[39m \u001b[39m123\u001b[39m,\n\u001b[1;32m    615\u001b[0m         stratify \u001b[39m=\u001b[39m train_and_validate[\u001b[39m\"\u001b[39m\u001b[39mmajor_category\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    617\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain shape: \u001b[39m\u001b[39m{\u001b[39;00mtrain\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2420\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2417\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2419\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2420\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2422\u001b[0m )\n\u001b[1;32m   2424\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2425\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2098\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2095\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2097\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2098\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2099\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2100\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2101\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2102\u001b[0m     )\n\u001b[1;32m   2104\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "train, validate, test = prepare.split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping outliers on train df\n",
    "train = prepare.percentile_capping(train, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running imputer func on train\n",
    "train_imputed = prepare.train_iterative_imputer(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing final imputation on validate and test dfs\n",
    "validate_imputed, test_imputed = prepare.impute_val_and_test(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b752499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total df shape: (48250, 32).\n",
      "Train shape: (27020, 32).\n",
      "Validate shape: (11580, 32).\n",
      "Test shape: (9650, 32).\n"
     ]
    }
   ],
   "source": [
    "# Checking shape on our samples to confirm appropriate split\n",
    "\n",
    "print('Imputed Train shape: {}.'.format(train_imputed.shape))\n",
    "print('Imputed Validate shape: {}.'.format(validate_imputed.shape))\n",
    "print('Imputed Test shape: {}.'.format(test_imputed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b89b11",
   "metadata": {},
   "source": [
    "## Set the Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16efb7",
   "metadata": {},
   "source": [
    "#### Note: Not all visuals, analysis, and work is shown within this Final Report. \n",
    "#### All my work, from start to finish, is available in my `working_notebook.ipynb` file for your reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01961c2",
   "metadata": {},
   "source": [
    "Our acquired and prepared dataset contains information for 48,250 homes. \\\n",
    "    In the process of exploring this data and setting initial hypotheses, I created a figure plotting choice categorical variables with our target variable of `logerror`. Using this figure I determined potential correlation with each of the features stated in my initial hypotheses. The following exploration seeks to answer these questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
